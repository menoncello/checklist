name: Performance Tests

on:
  pull_request:
    branches: [ main ]
    paths:
      - 'packages/**/*.ts'
      - 'packages/**/*.js'
      - 'package.json'
      - 'packages/*/package.json'
      - '.github/workflows/performance.yml'
  push:
    branches: [ main ]
    paths:
      - 'packages/**/*.ts'
      - 'packages/**/*.js'

env:
  BUN_VERSION: '1.1.x'
  NODE_ENV: 'test'
  PERFORMANCE_MONITORING: 'true'
  BENCHMARK_ENABLED: 'true'
  BENCHMARK_FAIL_ON_VIOLATION: 'true'
  BENCHMARK_VIOLATION_THRESHOLD: '110' # 10% over budget fails

jobs:
  performance-tests:
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 2 # For comparison with previous commit

      - name: Setup Bun
        uses: oven-sh/setup-bun@v1
        with:
          bun-version: ${{ env.BUN_VERSION }}

      - name: Cache dependencies
        uses: actions/cache@v3
        with:
          path: ~/.bun/install/cache
          key: ${{ runner.os }}-bun-${{ hashFiles('**/bun.lockb') }}
          restore-keys: |
            ${{ runner.os }}-bun-

      - name: Install dependencies
        run: bun install --frozen-lockfile

      - name: Build packages
        run: bun run build:all

      - name: Run performance benchmarks
        id: benchmarks
        run: |
          bun run bench:ci 2>&1 | tee benchmark-output.txt
          echo "benchmark_exit_code=$?" >> $GITHUB_OUTPUT
        continue-on-error: true

      - name: Parse benchmark results
        id: parse_results
        run: |
          # Extract key metrics from benchmark output
          if [ -f "benchmark-results.json" ]; then
            TOTAL_BENCHMARKS=$(jq -r '.summary.totalBenchmarks' benchmark-results.json)
            PASSED=$(jq -r '.summary.passed' benchmark-results.json)
            FAILED=$(jq -r '.summary.failed' benchmark-results.json)
            VIOLATIONS=$(jq -r '.summary.budgetViolations' benchmark-results.json)
            
            echo "total_benchmarks=$TOTAL_BENCHMARKS" >> $GITHUB_OUTPUT
            echo "passed=$PASSED" >> $GITHUB_OUTPUT
            echo "failed=$FAILED" >> $GITHUB_OUTPUT
            echo "violations=$VIOLATIONS" >> $GITHUB_OUTPUT
          else
            echo "total_benchmarks=0" >> $GITHUB_OUTPUT
            echo "passed=0" >> $GITHUB_OUTPUT
            echo "failed=0" >> $GITHUB_OUTPUT
            echo "violations=0" >> $GITHUB_OUTPUT
          fi

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: benchmark-results-${{ github.run_id }}
          path: |
            benchmark-results.json
            benchmark-output.txt
          retention-days: 30

      - name: Download baseline results
        uses: actions/download-artifact@v4
        if: github.event_name == 'pull_request'
        with:
          name: baseline-benchmark-results
          path: baseline/
        continue-on-error: true

      - name: Compare with baseline
        id: comparison
        if: github.event_name == 'pull_request' && hashFiles('baseline/benchmark-results.json') != ''
        run: |
          bun run bench:compare baseline/benchmark-results.json benchmark-results.json > comparison-report.txt
          REGRESSION_COUNT=$(grep -c "REGRESSION" comparison-report.txt || echo "0")
          echo "regression_count=$REGRESSION_COUNT" >> $GITHUB_OUTPUT
          echo "comparison_available=true" >> $GITHUB_OUTPUT
        continue-on-error: true

      - name: Create performance report
        id: report
        run: |
          cat << 'EOF' > performance-report.md
          ## 🎯 Performance Test Results

          ### Summary
          - **Total Benchmarks:** ${{ steps.parse_results.outputs.total_benchmarks }}
          - **Passed:** ${{ steps.parse_results.outputs.passed }}
          - **Failed:** ${{ steps.parse_results.outputs.failed }}
          - **Budget Violations:** ${{ steps.parse_results.outputs.violations }}
          
          ### Status
          EOF
          
          if [ "${{ steps.benchmarks.outputs.benchmark_exit_code }}" = "0" ]; then
            echo "✅ All performance tests passed!" >> performance-report.md
          else
            echo "❌ Performance tests failed!" >> performance-report.md
          fi
          
          if [ -f "benchmark-results.json" ] && [ "${{ steps.parse_results.outputs.violations }}" != "0" ]; then
            echo "" >> performance-report.md
            echo "### ⚠️ Budget Violations" >> performance-report.md
            jq -r '.violations[] | "- **\(.operation)**: \(.actual | tonumber | . * 100 | round / 100)ms (budget: \(.budget)ms, +\(.exceedance | tonumber | . * 100 | round / 100)%)"' benchmark-results.json >> performance-report.md
          fi
          
          if [ "${{ steps.comparison.outputs.comparison_available }}" = "true" ] && [ "${{ steps.comparison.outputs.regression_count }}" != "0" ]; then
            echo "" >> performance-report.md
            echo "### 📉 Performance Regressions" >> performance-report.md
            echo "Found ${{ steps.comparison.outputs.regression_count }} performance regressions compared to baseline." >> performance-report.md
            echo "" >> performance-report.md
            echo "<details>" >> performance-report.md
            echo "<summary>Regression Details</summary>" >> performance-report.md
            echo "" >> performance-report.md
            echo "```" >> performance-report.md
            cat comparison-report.txt >> performance-report.md
            echo "```" >> performance-report.md
            echo "" >> performance-report.md
            echo "</details>" >> performance-report.md
          fi
          
          # Add top slowest operations
          if [ -f "benchmark-results.json" ]; then
            echo "" >> performance-report.md
            echo "### 🐌 Slowest Operations (Top 10)" >> performance-report.md
            jq -r '.results | sort_by(.duration) | reverse | .[0:10][] | "- **\(.name)**: \(.duration | tonumber | . * 100 | round / 100)ms (\(.opsPerSecond | tonumber | . / 1000 | floor)K ops/sec)"' benchmark-results.json >> performance-report.md
          fi
          
          echo "" >> performance-report.md
          echo "### 📊 Artifacts" >> performance-report.md
          echo "- [Benchmark Results JSON](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})" >> performance-report.md
          echo "- [Full Benchmark Output](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})" >> performance-report.md

      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const report = fs.readFileSync('performance-report.md', 'utf8');
            
            // Find existing performance comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });
            
            const existingComment = comments.find(comment => 
              comment.body.includes('🎯 Performance Test Results')
            );
            
            if (existingComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: existingComment.id,
                body: report,
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: report,
              });
            }

      - name: Set job status
        if: always()
        run: |
          if [ "${{ steps.benchmarks.outputs.benchmark_exit_code }}" != "0" ]; then
            echo "❌ Performance tests failed"
            exit 1
          elif [ "${{ steps.comparison.outputs.regression_count || 0 }}" != "0" ]; then
            echo "❌ Performance regressions detected"
            exit 1
          else
            echo "✅ Performance tests passed"
            exit 0
          fi

  # Store baseline results for main branch
  store-baseline:
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    runs-on: ubuntu-latest
    needs: performance-tests

    steps:
      - name: Download benchmark results
        uses: actions/download-artifact@v4
        with:
          name: benchmark-results-${{ github.run_id }}

      - name: Upload as baseline
        uses: actions/upload-artifact@v4
        with:
          name: baseline-benchmark-results
          path: benchmark-results.json
          retention-days: 90

  # Generate performance trends report (weekly)
  performance-trends:
    if: github.event_name == 'schedule' || (github.event_name == 'workflow_dispatch')
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Bun
        uses: oven-sh/setup-bun@v1
        with:
          bun-version: ${{ env.BUN_VERSION }}

      - name: Install dependencies
        run: bun install --frozen-lockfile

      - name: Generate trends report
        run: |
          bun run bench:trends > performance-trends.md

      - name: Create issue for trends report
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const trends = fs.readFileSync('performance-trends.md', 'utf8');
            
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: `📊 Weekly Performance Trends Report - ${new Date().toISOString().split('T')[0]}`,
              body: trends,
              labels: ['performance', 'report', 'automation']
            });

# Schedule for weekly trends report (optional)
# on:
#   schedule:
#     - cron: '0 9 * * 1' # Every Monday at 9 AM UTC